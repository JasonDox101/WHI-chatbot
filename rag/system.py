from langgraph.graph import StateGraph, END
from typing import Dict, Any, List
from graph.state import WHIRAGState
from llm.qwen_client import QwenLLMClient
from vector_store.manager import WHIVectorStoreManager
from data.processor import WHIDataProcessor
from config.settings import WHIConfig
import json
import re
from datetime import datetime

class WHIRAGSystem:
    """WHI RAGç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self):
        self.llm_client = QwenLLMClient()
        self.vector_manager = WHIVectorStoreManager()
        self.data_processor = WHIDataProcessor()
        self.workflow = None
        self.conversation_memory = []  # å¯¹è¯è®°å¿†å­˜å‚¨
        self.max_history_length = 10  # æœ€å¤§ä¿å­˜å†å²å¯¹è¯æ•°é‡
        self._initialize_system()
        self._build_workflow()
    
    def _initialize_system(self) -> None:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            # åŠ è½½æ•°æ®
            self.data_processor.load_data()
            
            # å°è¯•åŠ è½½å·²å­˜åœ¨çš„å‘é‡å­˜å‚¨
            if not self.vector_manager.load_vector_store():
                print("Creating new vector store...") 
                documents = self.data_processor.create_documents()
                self.vector_manager.create_vector_store(documents)
                print("Vector store creation completed") 
            else:
                print("Vector store loaded successfully")  
        except Exception as e:
            print(f"System initialization failed: {str(e)}")  
            raise
    
    def process_question(self, question: str, conversation_history: List[Dict] = None) -> Dict[str, Any]:
        """å¤„ç†é—®é¢˜ï¼Œæ”¯æŒå¯¹è¯ä¸Šä¸‹æ–‡"""
        try:
            # åˆå§‹åŒ–çŠ¶æ€ï¼ŒåŒ…å«å¯¹è¯å†å²
            initial_state = {
                "question": question,
                "conversation_history": conversation_history or [],
                "processing_steps": []
            }
            
            # è¿è¡Œå·¥ä½œæµ
            result = self.workflow.invoke(initial_state)
            
            # ä¿å­˜åˆ°å¯¹è¯è®°å¿†
            self._save_to_memory(question, result)
            
            return result
        except Exception as e:
            return {
                "error": f"é—®é¢˜å¤„ç†å¤±è´¥: {str(e)}",
                "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ã€‚",
                "summary_answer": "ç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "confidence_score": 0.0,
                "sources": [],
                "processing_steps": [f"é”™è¯¯: {str(e)}"]
            }
    
    def _save_to_memory(self, question: str, result: Dict[str, Any]):
        """ä¿å­˜å¯¹è¯åˆ°è®°å¿†ä¸­"""
        try:
            qa_pair = {
                "question": question,
                "answer": result.get("summary_answer", ""),
                "detailed_answer": result.get("answer", ""),
                "timestamp": datetime.now().isoformat(),
                "confidence": result.get("confidence_score", 0),
                "question_type": result.get("question_type", "general")
            }
            
            self.conversation_memory.append(qa_pair)
            
            # ä¿æŒè®°å¿†é•¿åº¦é™åˆ¶
            if len(self.conversation_memory) > self.max_history_length:
                self.conversation_memory.pop(0)
        except Exception as e:
            print(f"ä¿å­˜å¯¹è¯è®°å¿†å¤±è´¥: {str(e)}")
    
    def _build_workflow(self) -> None:
        """æ„å»ºå¢å¼ºçš„LangGraphå·¥ä½œæµ"""
        workflow = StateGraph(WHIRAGState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_context", self._analyze_context)
        workflow.add_node("classify_question", self._classify_question)
        workflow.add_node("retrieve_documents", self._retrieve_documents)
        workflow.add_node("generate_answer", self._generate_answer)
        workflow.add_node("summarize_answer", self._summarize_answer)
        workflow.add_node("validate_answer", self._validate_answer)
        
        # è®¾ç½®è¾¹ - æ–°çš„æµç¨‹
        workflow.set_entry_point("analyze_context")
        workflow.add_edge("analyze_context", "classify_question")
        workflow.add_edge("classify_question", "retrieve_documents")
        workflow.add_edge("retrieve_documents", "generate_answer")
        workflow.add_edge("generate_answer", "summarize_answer")
        workflow.add_edge("summarize_answer", "validate_answer")
        workflow.add_edge("validate_answer", END)
        
        self.workflow = workflow.compile()
    
    def _analyze_context(self, state: WHIRAGState) -> Dict[str, Any]:
        """åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡çš„èŠ‚ç‚¹"""
        try:
            question = state["question"]
            history = state.get("conversation_history", [])
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹ä¸Šä¸‹æ–‡åˆ†æ")
            
            
            if not history:
                return {
                    "context_summary": "æ— å†å²å¯¹è¯ä¸Šä¸‹æ–‡",
                    "related_previous_qa": [],
                    "is_context_related": False,
                    "processing_steps": processing_steps
                }
            
            # æ”¹è¿›çš„ä¸Šä¸‹æ–‡åˆ†ææç¤º
            context_prompt = f"""
ä½ æ˜¯WHIåŒ»å­¦æ•°æ®åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æå½“å‰é—®é¢˜ä¸å†å²å¯¹è¯çš„å…³è”æ€§ã€‚

å½“å‰é—®é¢˜ï¼š{question}

å†å²å¯¹è¯ï¼š
{self._format_history_for_analysis(history)}

åˆ†æè¦æ±‚ï¼š
1. æ£€æŸ¥å½“å‰é—®é¢˜æ˜¯å¦å¼•ç”¨äº†ä¹‹å‰æåˆ°çš„æ¦‚å¿µã€æ•°å€¼ã€å˜é‡åç­‰
2. åˆ¤æ–­æ˜¯å¦éœ€è¦ç»“åˆä¹‹å‰çš„ç­”æ¡ˆæ¥å›ç­”å½“å‰é—®é¢˜
3. è¯†åˆ«ç›¸å…³çš„å†å²é—®ç­”å¯¹
4. ç”Ÿæˆç®€æ´çš„ä¸Šä¸‹æ–‡æ€»ç»“

è¯·è¿”å›JSONæ ¼å¼ï¼ˆç¡®ä¿æ ¼å¼æ­£ç¡®ï¼‰ï¼š
{{
    "is_related": true,
    "context_summary": "ç®€æ´çš„ä¸Šä¸‹æ–‡æ€»ç»“",
    "related_qa_indices": [0, 1],
    "reasoning": "è¯¦ç»†çš„å…³è”æ€§åˆ†æåŸå› "
}}
"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒ»å­¦æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡å…³è”æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚"},
                {"role": "user", "content": context_prompt}
            ]
            
            try:
                analysis_result = self.llm_client.generate_response(messages)
                # print(f"LLMä¸Šä¸‹æ–‡åˆ†æç»“æœ: {analysis_result}")  # ç§»é™¤æ­¤è¡Œ
                
                # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                if "```json" in analysis_result:
                    analysis_result = analysis_result.split("```json")[1].split("```")[0].strip()
                elif "```" in analysis_result:
                    analysis_result = analysis_result.split("```")[1].strip()
                
                analysis = json.loads(analysis_result)
            except Exception as e:
                # print(f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åŒ¹é…: {e}")  # ç§»é™¤æ­¤è¡Œ
                # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„å…³é”®è¯åŒ¹é…
                analysis = self._enhanced_context_analysis(question, history)
            
            # æå–ç›¸å…³çš„å†å²é—®ç­”
            related_qa = []
            if analysis.get("is_related", False):
                for idx in analysis.get("related_qa_indices", []):
                    if 0 <= idx < len(history):
                        related_qa.append({
                            "question": history[idx]["question"],
                            "answer": history[idx]["answer"]
                        })
            
            processing_steps.append("ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ")
            
            return {
                "context_summary": analysis.get("context_summary", ""),
                "related_previous_qa": related_qa,
                "is_context_related": analysis.get("is_related", False),
                "processing_steps": processing_steps
            }
            
        except Exception as e:
            return {
                "context_summary": "ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥",
                "related_previous_qa": [],
                "is_context_related": False,
                "error": f"ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {str(e)}",
                "processing_steps": processing_steps + [f"ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {str(e)}"]
            }
    
    def _simple_context_analysis(self, question: str, history: List[Dict]) -> Dict[str, Any]:
        """ç®€å•çš„ä¸Šä¸‹æ–‡åˆ†æï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
        question_lower = question.lower()
        related_indices = []
        
        for i, item in enumerate(history):
            hist_question = item.get("question", "").lower()
            # ç®€å•çš„å…³é”®è¯é‡å æ£€æµ‹
            question_words = set(question_lower.split())
            hist_words = set(hist_question.split())
            overlap = len(question_words & hist_words)
            
            if overlap > 1:  # å¦‚æœæœ‰è¶…è¿‡1ä¸ªè¯é‡å 
                related_indices.append(i)
        
        return {
            "is_related": len(related_indices) > 0,
            "context_summary": f"å‘ç°{len(related_indices)}ä¸ªç›¸å…³å†å²å¯¹è¯" if related_indices else "æ— ç›¸å…³å†å²å¯¹è¯",
            "related_qa_indices": related_indices,
            "reasoning": "åŸºäºå…³é”®è¯åŒ¹é…çš„ç®€å•åˆ†æ"
        }
    
    def _classify_question(self, state: WHIRAGState) -> Dict[str, Any]:
        """é—®é¢˜åˆ†ç±»èŠ‚ç‚¹"""
        try:
            question = state["question"]
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹é—®é¢˜åˆ†ç±»")
            
            classification_prompt = f"""
Please classify the following question about WHI (Women's Health Initiative) data:

Question: {question}

Classify into one of these categories:
- "variable": Questions about specific variables, measurements, or data fields
- "dataset": Questions about datasets, studies, or data collection methods
- "general": General questions about WHI research, methodology, or interpretation

Return only the category name.
"""
            
            messages = [
                {"role": "system", "content": "You are a professional medical data analysis assistant."},
                {"role": "user", "content": classification_prompt}
            ]
            
            question_type = self.llm_client.generate_response(messages).strip().lower()
            
            # ç¡®ä¿åˆ†ç±»ç»“æœæœ‰æ•ˆ
            if question_type not in ["variable", "dataset", "general"]:
                question_type = "general"
            
            processing_steps.append(f"é—®é¢˜åˆ†ç±»å®Œæˆ: {question_type}")
            
            return {
                "question_type": question_type,
                "processing_steps": processing_steps
            }
        except Exception as e:
            return {
                "question_type": "general",
                "error": f"é—®é¢˜åˆ†ç±»å¤±è´¥: {str(e)}",
                "processing_steps": processing_steps + [f"é—®é¢˜åˆ†ç±»å¤±è´¥: {str(e)}"]
            }
    
    def _retrieve_documents(self, state: WHIRAGState) -> Dict[str, Any]:
        """æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹"""
        try:
            question = state["question"]
            question_type = state.get("question_type", "general")
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹æ–‡æ¡£æ£€ç´¢")
            
            # ç”Ÿæˆä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢
            search_query = self._generate_search_query(question, question_type)
            processing_steps.append(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢: {search_query}")
            
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            retrieved_docs = self.vector_manager.similarity_search(
                search_query, 
                k=WHIConfig.RETRIEVAL_K
            )
            
            processing_steps.append(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            return {
                "search_query": search_query,
                "retrieved_documents": retrieved_docs,
                "processing_steps": processing_steps
            }
        except Exception as e:
            return {
                "error": f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}",
                "retrieved_documents": [],
                "processing_steps": processing_steps + [f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}"]
            }
    
    def _generate_search_query(self, question: str, question_type: str) -> str:
        """ç”Ÿæˆä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢"""
        try:
            query_prompt = f"""
Based on question type "{question_type}" and user question, generate keyword queries suitable for retrieval in WHI medical data.

User question: {question}

Please extract key medical terms, variable names, or dataset names to generate concise search queries.
Return only the search query, without any other content.
"""
            
            messages = [
                {"role": "system", "content": "You are a professional medical research assistant."},
                {"role": "user", "content": query_prompt}
            ]
            
            return self.llm_client.generate_response(messages).strip()
        except:
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸå§‹é—®é¢˜
            return question
    
    def _generate_answer(self, state: WHIRAGState) -> Dict[str, Any]:
        """å¢å¼ºçš„ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹ - æ”¯æŒä¸Šä¸‹æ–‡"""
        try:
            question = state["question"]
            retrieved_docs = state.get("retrieved_documents", [])
            related_qa = state.get("related_previous_qa", [])
            context_summary = state.get("context_summary", "")
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­”æ¡ˆç”Ÿæˆ")
            
            # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
            context = self._build_context(retrieved_docs)
            
            # æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„prompt
            context_info = ""
            if related_qa:
                context_info = "\n\n**ç›¸å…³å†å²å¯¹è¯ï¼š**\n"
                for i, qa in enumerate(related_qa, 1):
                    context_info += f"{i}. Q: {qa['question']}\n   A: {qa['answer']}\n\n"
            
            if context_summary and context_summary != "æ— å†å²å¯¹è¯ä¸Šä¸‹æ–‡":
                context_info += f"\n**å¯¹è¯ä¸Šä¸‹æ–‡æ€»ç»“ï¼š**\n{context_summary}\n\n"
            
            # ğŸ”§ ä¿®æ”¹promptï¼Œåªçº¦æŸæ ¼å¼ï¼Œä¿æŒå†…å®¹é£æ ¼
            enhanced_prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„WHIåŒ»å­¦æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£ä¸Šä¸‹æ–‡å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·å½“å‰é—®é¢˜ï¼š{question}

æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼š
{context}
{context_info}

**é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹markdownæ ¼å¼è¦æ±‚è¾“å‡ºï¼Œä½†ä¿æŒä½ åŸæœ‰çš„ä¸“ä¸šå›ç­”é£æ ¼å’Œå†…å®¹æ·±åº¦ï¼š**

1. **æ ‡é¢˜æ ¼å¼**ï¼šä½¿ç”¨ ## ä½œä¸ºä¸»æ ‡é¢˜ï¼Œ### ä½œä¸ºå­æ ‡é¢˜
2. **åˆ—è¡¨æ ¼å¼**ï¼šä½¿ç”¨ - å¼€å¤´çš„æ— åºåˆ—è¡¨ï¼Œæˆ– 1. å¼€å¤´çš„æœ‰åºåˆ—è¡¨
3. **å¼ºè°ƒæ ¼å¼**ï¼šé‡è¦ä¿¡æ¯ç”¨ **ç²—ä½“** æ ‡è®°
4. **æ•°å€¼æ ¼å¼**ï¼šå…·ä½“æ•°å€¼å’Œå•ä½ç”¨ **ç²—ä½“** çªå‡ºæ˜¾ç¤º
5. **æ®µè½æ ¼å¼**ï¼šæ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”

è¯·ä¿æŒä½ ä¸€è´¯çš„ï¼š
- å‡†ç¡®ã€ä¸“ä¸šçš„åŒ»å­¦æœ¯è¯­ä½¿ç”¨
- å‡†ç¡®ã€ä¸“ä¸šçš„ç­”æ¡ˆ
- å¦‚æœä¸å†å²å¯¹è¯ç›¸å…³ï¼Œè¯·é€‚å½“å¼•ç”¨å’Œå…³è”
- è¯¦ç»†çš„æ•°æ®åˆ†æå’Œè§£é‡Š
- å®¢è§‚çš„å­¦æœ¯è¯­è°ƒ
- ä¸°å¯Œçš„èƒŒæ™¯ä¿¡æ¯æä¾›
- åŒ…å«å…·ä½“çš„æ•°æ®å’ŒæŒ‡æ ‡

æ³¨æ„ï¼šå¦‚æœå½“å‰é—®é¢˜ä¸ä¹‹å‰çš„é—®é¢˜ç›¸å…³ï¼Œè¯·åœ¨ç­”æ¡ˆä¸­ä½“ç°è¿™ç§å…³è”æ€§ã€‚

ç¡®ä¿è¾“å‡ºæ ¼å¼æ ‡å‡†åŒ–ã€‚
"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒ»å­¦æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»“åˆå†å²å¯¹è¯ä¸Šä¸‹æ–‡æä¾›å‡†ç¡®ç­”æ¡ˆã€‚è¯·ä¸¥æ ¼éµå¾ªmarkdownæ ¼å¼è¦æ±‚ï¼Œä½†ä¿æŒä¸“ä¸šçš„å›ç­”é£æ ¼ã€‚"},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            answer = self.llm_client.generate_response(messages)
            
            # ğŸ”§ åº”ç”¨markdownæ ¼å¼æ ‡å‡†åŒ–
            answer = self._ensure_markdown_format(answer)
            
            # æå–æºä¿¡æ¯
            sources = self._extract_sources(retrieved_docs)
            
            processing_steps.append("ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            return {
                "context": context,
                "answer": answer,
                "sources": sources,
                "processing_steps": processing_steps
            }
            
        except Exception as e:
            return {
                "error": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}",
                "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚",
                "processing_steps": processing_steps + [f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"]
            }
    
    def _build_context(self, documents: List) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not documents:
            return "No relevant information found."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"Information {i}:\n{doc.page_content}\n")
        
        return "\n".join(context_parts)
    
    def _extract_sources(self, documents: List) -> List[Dict[str, Any]]:
        """æå–æºä¿¡æ¯"""
        sources = []
        for doc in documents:
            source_info = {
                "type": doc.metadata.get("type", "unknown"),
                "dataset_name": doc.metadata.get("dataset_name", "N/A"),
                "variable_name": doc.metadata.get("variable_name", "N/A"),
                "study": doc.metadata.get("study", "N/A")
            }
            sources.append(source_info)
        return sources
    
    def _summarize_answer(self, state: WHIRAGState) -> Dict[str, Any]:
        """ç­”æ¡ˆæ€»ç»“èŠ‚ç‚¹ - ç¬¬äºŒä¸ªLLMè°ƒç”¨"""
        try:
            detailed_answer = state.get("answer", "")
            question = state["question"]
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹ç­”æ¡ˆæ€»ç»“")
            
            if not detailed_answer:
                return {
                    "summary_answer": "æ— æ³•ç”Ÿæˆç­”æ¡ˆæ€»ç»“",
                    "processing_steps": processing_steps + ["è¯¦ç»†ç­”æ¡ˆä¸ºç©ºï¼Œæ— æ³•æ€»ç»“"]
                }
            
            # æ€»ç»“prompt
            summary_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹è¯¦ç»†ç­”æ¡ˆæ€»ç»“ä¸ºç®€æ´ã€æ˜“æ‡‚çš„å›å¤ï¼Œé€‚åˆåœ¨èŠå¤©ç•Œé¢ä¸­æ˜¾ç¤ºã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯¦ç»†ç­”æ¡ˆï¼š
{detailed_answer}

è¯·æä¾›ï¼š
1. æ ¸å¿ƒè¦ç‚¹çš„ç®€æ´æ€»ç»“ï¼ˆ2-3å¥è¯ï¼‰
2. å…³é”®ä¿¡æ¯çš„æç‚¼
3. ä¿æŒä¸“ä¸šæ€§ä½†æ˜“äºç†è§£

æ€»ç»“åº”è¯¥ç®€æ´æ˜äº†ï¼Œé•¿åº¦æ§åˆ¶åœ¨100-200å­—ä»¥å†…ã€‚
"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿å°†å¤æ‚çš„åŒ»å­¦ä¿¡æ¯æ€»ç»“ä¸ºç®€æ´æ˜“æ‡‚çš„å†…å®¹ã€‚"},
                {"role": "user", "content": summary_prompt}
            ]
            
            summary_answer = self.llm_client.generate_response(messages)
            
            processing_steps.append("ç­”æ¡ˆæ€»ç»“å®Œæˆ")
            
            return {
                "summary_answer": summary_answer,
                "processing_steps": processing_steps
            }
        except Exception as e:
            return {
                "summary_answer": "ç­”æ¡ˆæ€»ç»“å¤±è´¥",
                "error": f"ç­”æ¡ˆæ€»ç»“å¤±è´¥: {str(e)}",
                "processing_steps": processing_steps + [f"ç­”æ¡ˆæ€»ç»“å¤±è´¥: {str(e)}"]
            }
    
    def _validate_answer(self, state: WHIRAGState) -> Dict[str, Any]:
        """ç­”æ¡ˆéªŒè¯èŠ‚ç‚¹"""
        try:
            answer = state.get("answer", "")
            sources = state.get("sources", [])
            processing_steps = state.get("processing_steps", [])
            processing_steps.append("å¼€å§‹ç­”æ¡ˆéªŒè¯")
            
            # ç®€å•çš„ç½®ä¿¡åº¦è¯„ä¼°
            confidence_score = self._calculate_confidence(answer, sources)
            
            processing_steps.append(f"ç­”æ¡ˆéªŒè¯å®Œæˆï¼Œç½®ä¿¡åº¦: {confidence_score:.2f}")
            
            return {
                "confidence_score": confidence_score,
                "processing_steps": processing_steps
            }
        except Exception as e:
            return {
                "confidence_score": 0.0,
                "error": f"ç­”æ¡ˆéªŒè¯å¤±è´¥: {str(e)}",
                "processing_steps": processing_steps + [f"ç­”æ¡ˆéªŒè¯å¤±è´¥: {str(e)}"]
            }
    
    def _calculate_confidence(self, answer: str, sources: List[Dict[str, Any]]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        if not answer or not sources:
            return 0.0
        
        # åŸºäºç­”æ¡ˆé•¿åº¦å’Œæºæ•°é‡çš„ç®€å•ç½®ä¿¡åº¦è®¡ç®—
        answer_length_score = min(len(answer) / 500, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
        source_count_score = min(len(sources) / 5, 1.0)    # æ ‡å‡†åŒ–åˆ°0-1
        
        # ç»¼åˆè¯„åˆ†
        confidence = (answer_length_score * 0.6 + source_count_score * 0.4)
        return round(confidence, 2)
    
    def _format_history_for_analysis(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å†å²å¯¹è¯ç”¨äºåˆ†æ"""
        formatted = ""
        for i, item in enumerate(history[-5:], 1):  # åªå–æœ€è¿‘5æ¡
            formatted += f"{i}. Q: {item.get('question', '')}\n   A: {item.get('answer', '')}\n\n"
        return formatted


    def _ensure_markdown_format(self, answer: str) -> str:
        """ç¡®ä¿ç­”æ¡ˆç¬¦åˆæ ‡å‡†markdownæ ¼å¼ï¼Œä½†ä¸æ”¹å˜å†…å®¹"""
        import re
        
        # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ ‡å‡†åŒ–
        answer = re.sub(r'^#{1,6}\s*(.+)$', lambda m: f"## {m.group(1).strip()}", answer, flags=re.MULTILINE)
        
        # ç¡®ä¿åˆ—è¡¨æ ¼å¼æ ‡å‡†åŒ–
        answer = re.sub(r'^[â€¢Â·*]\s*', '- ', answer, flags=re.MULTILINE)
        
        # ç¡®ä¿æ®µè½é—´è·
        answer = re.sub(r'\n{3,}', '\n\n', answer)
        
        # ç¡®ä¿æ•°å€¼åŠ ç²—ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
        answer = re.sub(r'(?<!\*)\b(\d+(?:\.\d+)?\s*(?:g/dL|mg/dL|mmHg|%|å¹´|å²|ä¾‹|äºº|é¡¹))(?!\*)', r'**\1**', answer)
        
        return answer.strip()